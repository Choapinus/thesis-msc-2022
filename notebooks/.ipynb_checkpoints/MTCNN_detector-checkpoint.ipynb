{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c75521-dbc1-48a0-8a74-3e1fd6c4e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref detector: https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49\n",
    "# ref rotation: https://www.kaggle.com/code/gpiosenka/align-crop-resize-save-images-using-mtcnn\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../tflow/mtcnn')\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils.paths import list_images\n",
    "from scipy.spatial.distance import euclidean\n",
    "from utils import load_image, align, crop_image, rotate_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f3d3ad-75a1-4a25-9bd8-597f3c8c299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. obj detection\n",
    "# obj: usar mtcnn para detectar regiones de interes y guardarlas en un archivo json\n",
    "# este preprocesado ayudará en las siguientes etapas.\n",
    "\n",
    "# 2. make splits\n",
    "# make splits for each dataset conjunction\n",
    "# extra: separate them into jsons/txt\n",
    "# - flickr vs flickr\n",
    "# - splunk vs splunk\n",
    "# - flickr vs splunk\n",
    "# - splunk vs flickr\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'flickr': '/media/choppy/WD_BLACK/datasets/FLICKR',\n",
    "    'splunk': '/media/choppy/WD_BLACK/datasets/Splunk',\n",
    "}\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f16d119-6a23-4071-8dad-07f60ced8bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 11:55:01.561196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:01.900558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:01.902580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:01.906770: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 11:55:01.909413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:01.912005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:01.913586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:07.856671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:07.858299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:07.859823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-04 11:55:07.860825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4631 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# instance detector\n",
    "# TODO: explore min and max face size of detector inference\n",
    "# detector = MTCNN(min_face_size=400)\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda4ef32-2651-476c-b29b-6fbcf29c8487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing inference over 14000 images from FLICKR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   0%|                                                                                                                                                                            | 0/14000 [00:00<?, ?it/s]2022-10-04 11:55:12.575515: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8401\n",
      "2022-10-04 11:55:16.062555: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-04 11:55:16.063065: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-04 11:55:16.063084: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-10-04 11:55:16.063391: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-04 11:55:16.063426: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "flickr:   0%|▎                                                                                                                                                                | 27/14000 [00:38<2:42:44,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than one face detected in img: /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/originales_ronda3y4/F20369.png, but only the biggest is stored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   0%|▌                                                                                                                                                                | 46/14000 [00:54<2:50:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda4/F_screen_tablet_mano/F20305_screen_tablet_mano_20220227014143.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   1%|█▍                                                                                                                                                              | 121/14000 [01:47<2:09:25,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_print_normal_plano/F16867_print_normal_plano_20220219192808.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   1%|██▎                                                                                                                                                             | 197/14000 [02:44<2:49:51,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than one face detected in img: /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_print_brillante_plano/F12902_print_brillante_plano_20220328211322.jpg, but only the biggest is stored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   1%|██▎                                                                                                                                                             | 201/14000 [02:47<2:53:23,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda4/F_screen_smartphone_mano/F16325_screen_smartphone_mano_20220505044956.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   2%|██▍                                                                                                                                                             | 215/14000 [02:58<3:27:20,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than one face detected in img: /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_print_brillante_plano/F16582_print_brillante_plano_20220329195620.jpg, but only the biggest is stored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   2%|██▋                                                                                                                                                             | 239/14000 [03:16<2:57:42,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than one face detected in img: /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_screen_tablet_fijo/F24187_screen_tablet_fijo_20220219000718.jpg, but only the biggest is stored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   2%|██▉                                                                                                                                                             | 261/14000 [03:39<4:03:07,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda4/F_screen_tablet_mano/F16858_screen_tablet_mano_20220226235251.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   2%|███▎                                                                                                                                                            | 286/14000 [04:03<4:03:44,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_print_brillante_plano/F12409_print_brillante_plano_20220326032916.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   2%|███▌                                                                                                                                                            | 315/14000 [04:27<2:57:27,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_screen_tablet_fijo/F20194_screen_tablet_fijo_20220217224756.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   2%|███▊                                                                                                                                                            | 334/14000 [04:42<2:46:38,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_print_brillante_plano/F12690_print_brillante_plano_20220327011332.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   3%|████▌                                                                                                                                                           | 394/14000 [05:36<4:16:07,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_screen_smartphone_fijo/F12157_screen_smartphone_fijo_20220307014104.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   3%|████▊                                                                                                                                                           | 422/14000 [05:59<2:53:06,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_screen_computador_fijo/F04961_screen_computador_fijo_20220313220502.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   3%|████▉                                                                                                                                                           | 432/14000 [06:08<3:20:13,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda4/F_print_normal_curvado/F20805_print_normal_curvado_20220302012317.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   3%|█████▏                                                                                                                                                          | 453/14000 [06:26<3:36:14,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no predictions for /media/choppy/WD_BLACK/datasets/FLICKR/ronda3/F_print_brillante_plano/F12157_print_brillante_plano_20220326000916.jpg, please check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   3%|█████▏                                                                                                                                                          | 454/14000 [06:28<3:13:15,  1.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoing inference over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imdir \u001b[38;5;129;01min\u001b[39;00m tqdm(images, desc\u001b[38;5;241m=\u001b[39mdst_key):\n\u001b[0;32m---> 14\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     downscale_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     16\u001b[0m     original_shape \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/TOC/thesis-msc-2022/notebooks/utils.py:20\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(path, colorspace)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColorspace setted to RGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m     color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 20\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m, spaces[color])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# i = 0\n",
    "\n",
    "for dst_key in datasets:\n",
    "    ds_dir = datasets.get(dst_key)\n",
    "    # images = list(sorted([*list_images(ds_dir)], reverse=True))\n",
    "    images = [*list_images(ds_dir)]\n",
    "    np.random.shuffle(images)\n",
    "    db_name = os.path.basename(ds_dir)\n",
    "    json_data = []\n",
    "    \n",
    "    print(f\"doing inference over {len(images)} images from {db_name}\")\n",
    "    \n",
    "    for imdir in tqdm(images, desc=dst_key):\n",
    "        img = load_image(imdir)\n",
    "        downscale_factor = 4\n",
    "        original_shape = img.shape[:2]\n",
    "        _shape = np.array(img.shape[:2]) // downscale_factor\n",
    "        img = cv2.resize(img, _shape)\n",
    "        detections = detector.detect_faces(img)\n",
    "        img_path_dir = imdir.split(db_name)[-1][1:]\n",
    "        \n",
    "        # find best detection and biggest bbox\n",
    "        biggest = 0\n",
    "        best_det = None\n",
    "        if len(detections) > 1:\n",
    "            if verbose: print(f'more than one face detected in img: {imdir}, but only the biggest is stored')\n",
    "            for det in detections:\n",
    "                box = det['box']            \n",
    "                # calculate the area in the image\n",
    "                area = box[2] * box[3]\n",
    "                if area > biggest:\n",
    "                    biggest = area\n",
    "                    bbox = box\n",
    "                    best_det = det\n",
    "        elif len(detections) == 1:\n",
    "            best_det = detections[0]\n",
    "        else:\n",
    "            if verbose: print(f'no predictions for {imdir}, please check.')\n",
    "            continue\n",
    "        \n",
    "        # continue working with best_det dict\n",
    "        # scale up data from best_det\n",
    "        best_det['box'] = (np.array(best_det['box']) * downscale_factor).tolist()\n",
    "        for bkey in best_det['keypoints'].keys():\n",
    "            best_det['keypoints'][bkey] = (np.array(best_det['keypoints'][bkey]) * downscale_factor).tolist()\n",
    "        \n",
    "        red = [255, 0, 0]\n",
    "        bbox = best_det['box']\n",
    "        nose = best_det.get('keypoints')['nose']\n",
    "        left_eye, right_eye = best_det.get('keypoints')['left_eye'], best_det.get('keypoints')['right_eye']\n",
    "        dst1, dst2 = euclidean(left_eye, nose), euclidean(right_eye, nose)\n",
    "        mean_dst = np.mean([dst1, dst2]).astype(np.uint16)\n",
    "\n",
    "        # upscale image to checkout method\n",
    "        img = cv2.resize(img, original_shape)\n",
    "        periocular = img.copy()\n",
    "        \n",
    "        pt1 = (bbox[0], left_eye[1]-int(mean_dst*0.6))\n",
    "        pt2 = (bbox[0]+bbox[2], right_eye[1]+int(mean_dst*0.6))\n",
    "        \n",
    "        # periocular = periocular[ pt1[1]:pt2[1], pt1[0]:pt2[0], ... ] # use the generated points to crop the ROI\n",
    "\n",
    "        # face + distance\n",
    "        # periocular = cv2.rectangle(periocular, pt1, pt2, color=red, thickness=50)\n",
    "        \n",
    "        # plt.figure(figsize=(10, 8))\n",
    "        # plt.imshow(periocular)\n",
    "        # \n",
    "        # i += 1\n",
    "        # \n",
    "        # if i == 5:\n",
    "        #     break\n",
    "            \n",
    "        # make dict data with periocular region\n",
    "        peri_data = {\n",
    "            'image_dir': os.path.join(db_name, img_path_dir),\n",
    "            'mtcnn-inference': best_det, \n",
    "            'handcrafted': {\n",
    "                'periocular': [pt1[1], pt2[1], pt1[0], pt2[0]], # y2, y1, x2, x1\n",
    "                'description': 'crop of full size image with following format [y2, y1, x2, x1]. This new region was obtained calculing the 60% of euclidean distance between l/r eye and nose, by this way we get y-axis location, and x-axis location correspond to boundingbox xy detected by mtcnn'\n",
    "            }   \n",
    "        }\n",
    "        \n",
    "        json_data.append(peri_data)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # store json data as db-name.json\n",
    "    json.dump(json_data, open(db_name+'.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdd9e1-c3a1-4710-a4cc-f0b50caaf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para hacer zona periocular\n",
    "\n",
    "# distancia entre cada ojo y agregar 20% de margen izq/der\n",
    "# para altura, triangular distancia desde los ojos hasta la nariz y estimar un 20-30%\n",
    "\n",
    "\n",
    "# calcular distancia euclideana entre ambos puntos (nariz y ambos ojos) y calcular promedio\n",
    "# a ese promedio aplicarle 20-30% de margen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
