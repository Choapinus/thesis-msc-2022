{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c75521-dbc1-48a0-8a74-3e1fd6c4e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref detector: https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49\n",
    "# ref rotation: https://www.kaggle.com/code/gpiosenka/align-crop-resize-save-images-using-mtcnn\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../tflow/mtcnn')\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils.paths import list_images\n",
    "from scipy.spatial.distance import euclidean\n",
    "from utils import load_image, align, crop_image, rotate_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f3d3ad-75a1-4a25-9bd8-597f3c8c299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. obj detection\n",
    "# obj: usar mtcnn para detectar regiones de interes y guardarlas en un archivo json\n",
    "# este preprocesado ayudará en las siguientes etapas.\n",
    "\n",
    "# 2. make splits\n",
    "# make splits for each dataset conjunction\n",
    "# extra: separate them into jsons/txt\n",
    "# - flickr vs flickr\n",
    "# - splunk vs splunk\n",
    "# - flickr vs splunk\n",
    "# - splunk vs flickr\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'flickr': '/media/choppy/WD_BLACK/datasets/FLICKR',\n",
    "    'splunk': '/media/choppy/WD_BLACK/datasets/Splunk',\n",
    "}\n",
    "\n",
    "verbose = False\n",
    "debug = False\n",
    "save_steps = 10 # save data each N detections\n",
    "downscale_factor = 10 # used to downscale images and improve speed of mtcnn # seems not to be working. # Use wisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f16d119-6a23-4071-8dad-07f60ced8bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 16:52:24.970214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:25.787994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:25.789690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:25.813230: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-06 16:52:25.871373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:25.872121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:25.872772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:31.883012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:31.883368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:31.883674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-06 16:52:31.887276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4082 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# instance detector\n",
    "# TODO: explore min and max face size of detector inference\n",
    "# detector = MTCNN(min_face_size=400)\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda4ef32-2651-476c-b29b-6fbcf29c8487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing inference over 14000 images from FLICKR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flickr:   0%|                                                                                                                                                                            | 0/14000 [00:00<?, ?it/s]2022-10-06 16:52:42.972768: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8401\n",
      "2022-10-06 16:52:50.858636: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-06 16:52:50.859692: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-06 16:52:50.859777: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-10-06 16:52:50.860972: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-06 16:52:50.861165: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "flickr:  26%|█████████████████████████████████████████▌                                                                                                                     | 3657/14000 [46:58<1:50:40,  1.56it/s]Premature end of JPEG file\n",
      "flickr: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14000/14000 [2:59:57<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing inference over 24998 images from Splunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splunk: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24998/24998 [5:29:32<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for dst_key in datasets:\n",
    "    ds_dir = datasets.get(dst_key)\n",
    "    # images = list(sorted([*list_images(ds_dir)], reverse=True))\n",
    "    images = [*list_images(ds_dir)]\n",
    "    # np.random.shuffle(images)\n",
    "    db_name = os.path.basename(ds_dir)\n",
    "    json_data = []\n",
    "    \n",
    "    print(f\"doing inference over {len(images)} images from {db_name}\")\n",
    "    \n",
    "    for imdir in tqdm(images, desc=dst_key):\n",
    "        img = load_image(imdir)\n",
    "        \n",
    "        if img is None:\n",
    "            if verbose: print(f'img {imdir} could not be loaded. Check')\n",
    "            continue\n",
    "        \n",
    "        original_shape = img.shape[:2]\n",
    "        _shape = np.array(img.shape[:2]) // downscale_factor\n",
    "        img = cv2.resize(img, _shape[::-1])\n",
    "        detections = detector.detect_faces(img)\n",
    "        img_path_dir = imdir.split(db_name)[-1][1:]\n",
    "        \n",
    "        # find best detection and biggest bbox\n",
    "        biggest = 0\n",
    "        best_det = None\n",
    "        if len(detections) > 1:\n",
    "            if verbose: print(f'more than one face detected in img: {imdir}, but only the biggest is stored')\n",
    "            for det in detections:\n",
    "                box = det['box']            \n",
    "                # calculate the area in the image\n",
    "                area = box[2] * box[3]\n",
    "                if area > biggest:\n",
    "                    biggest = area\n",
    "                    bbox = box\n",
    "                    best_det = det\n",
    "        elif len(detections) == 1:\n",
    "            best_det = detections[0]\n",
    "        else:\n",
    "            if verbose: print(f'no predictions for {imdir}, please check.')\n",
    "            continue\n",
    "        \n",
    "        # continue working with best_det dict\n",
    "        # scale up data from best_det\n",
    "        best_det['box'] = (np.array(best_det['box']) * downscale_factor).tolist()\n",
    "        for bkey in best_det['keypoints'].keys():\n",
    "            best_det['keypoints'][bkey] = (np.array(best_det['keypoints'][bkey]) * downscale_factor).tolist()\n",
    "        \n",
    "        red = [255, 0, 0]\n",
    "        bbox = best_det['box']\n",
    "        nose = best_det.get('keypoints')['nose']\n",
    "        left_eye, right_eye = best_det.get('keypoints')['left_eye'], best_det.get('keypoints')['right_eye']\n",
    "        dst1, dst2 = euclidean(left_eye, nose), euclidean(right_eye, nose)\n",
    "        mean_dst = np.mean([dst1, dst2]).astype(np.uint16)\n",
    "\n",
    "        # upscale image to checkout method\n",
    "        img = cv2.resize(img, original_shape[::-1])\n",
    "        periocular = img.copy()\n",
    "        \n",
    "        pt1 = (bbox[0], left_eye[1]-int(mean_dst*0.6))\n",
    "        pt2 = (bbox[0]+bbox[2], right_eye[1]+int(mean_dst*0.6))\n",
    "        \n",
    "        if debug:\n",
    "            periocular = periocular[ pt1[1]:pt2[1], pt1[0]:pt2[0], ... ] # use the generated points to crop the ROI\n",
    "\n",
    "            # face + distance\n",
    "            img = cv2.rectangle(img, pt1, pt2, color=red, thickness=50)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(periocular)\n",
    "            plt.show()\n",
    "            \n",
    "            if i == 5:\n",
    "                break\n",
    "            \n",
    "        # make dict data with periocular region\n",
    "        peri_data = {\n",
    "            'image_dir': os.path.join(db_name, img_path_dir),\n",
    "            'mtcnn-inference': best_det, \n",
    "            'handcrafted': {\n",
    "                'periocular': [ pt1[1], pt2[1], pt1[0], pt2[0] ], # y2, y1, x2, x1 \n",
    "                # 'description': 'crop of full size image with following format [y2, y1, x2, x1]. This new region was obtained calculing the 60% of euclidean distance between l/r eye and nose, by this way we get y-axis location, and x-axis location correspond to boundingbox xy detected by mtcnn'\n",
    "            }   \n",
    "        }\n",
    "        \n",
    "        \n",
    "        json_data.append(peri_data)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # save data per steps\n",
    "        if i % save_steps == 0:\n",
    "            json.dump(json_data, open(db_name+'.json', 'w'))\n",
    "        \n",
    "        \n",
    "    \n",
    "    # store json data as db-name.json\n",
    "    json.dump(json_data, open(db_name+'.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbbdd9e1-c3a1-4710-a4cc-f0b50caaf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para hacer zona periocular\n",
    "\n",
    "# distancia entre cada ojo y agregar 20% de margen izq/der\n",
    "# para altura, triangular distancia desde los ojos hasta la nariz y estimar un 20-30%\n",
    "\n",
    "\n",
    "# calcular distancia euclideana entre ambos puntos (nariz y ambos ojos) y calcular promedio\n",
    "# a ese promedio aplicarle 20-30% de margen\n",
    "\n",
    "# all done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628fd6a0-96f8-4b7f-ba63-5e32db905ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
